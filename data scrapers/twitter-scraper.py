"""
sÃ¼rdÃ¼rÃ¼lebilir ÅŸehirler hackathonu

orbit

tuÄŸrap efe dikpÄ±nar
ahmet mert tezcan

tweet scraper :p
"""


#zaman aralÄ±klarÄ± bir algortimaya oturtullmalÄ±

#komutanÄ±m beni soÄŸuk deÄŸil sizin mont sÃ¶zÃ¼nÃ¼z Ã¶ldÃ¼rdÃ¼

import time
import json
import random
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from fake_useragent import UserAgent
import re
import os
from dotenv import load_dotenv


# istanbul ankara izmir ikÃ§e olarak eklenebilir ya da bÃ¼yÃ¼k ilÃ§eler konya ereÄŸli falan
SEHIRLER = [
    'Adana','AdÄ±yaman','Afyonkarahisar','AÄŸrÄ±','Amasya','Ankara','Antalya','Artvin','AydÄ±n','BalÄ±kesir',
    'Bilecik','BingÃ¶l','Bitlis','Bolu','Burdur','Bursa','Ã‡anakkale','Ã‡ankÄ±rÄ±','Ã‡orum','Denizli',
    'DiyarbakÄ±r','Edirne','ElazÄ±ÄŸ','Erzincan','Erzurum','EskiÅŸehir','Gaziantep','Giresun','GÃ¼mÃ¼ÅŸhane','Hakkari',
    'Hatay','Isparta','Mersin','Ä°stanbul','Ä°zmir','Kars','Kastamonu','Kayseri','KÄ±rklareli','KÄ±rÅŸehir',
    'Kocaeli','Konya','KÃ¼tahya','Malatya','Manisa','KahramanmaraÅŸ','Mardin','MuÄŸla','MuÅŸ','NevÅŸehir',
    'NiÄŸde','Ordu','Rize','Sakarya','Samsun','Siirt','Sinop','Sivas','TekirdaÄŸ','Tokat',
    'Trabzon','Tunceli','ÅanlÄ±urfa','UÅŸak','Van','Yozgat','Zonguldak','Aksaray','Bayburt','Karaman',
    'KÄ±rÄ±kkale','Batman','ÅÄ±rnak','BartÄ±n','Ardahan','IÄŸdÄ±r','Yalova','KarabÃ¼k','Kilis','Osmaniye',
    'DÃ¼zce'
]


_sorted_cities = sorted(SEHIRLER, key=len, reverse=True)
_city_pattern = re.compile(r"\b(" + "|".join(map(re.escape, _sorted_cities)) + r")(?:'?[dDbB][ea]|'?[dDtT][an]|'?[yY][ae])?\b", re.IGNORECASE)


def detect_city_from_text(text: str) -> str | None:

    if not text:
        return None
    # Basit normalize
    candidate = text.strip()
    m = _city_pattern.search(candidate)
    if m:
        # Orijinal ÅŸehir adÄ±nÄ± bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k/aksan aÃ§Ä±sÄ±ndan normalize et
        found = m.group(1)
        # Listeden tam eÅŸleÅŸen standard biÃ§imi dÃ¶ndÃ¼r
        for c in SEHIRLER:
            if c.lower() == found.lower():
                return c
        return found
    return None


class SeleniumTwitterScraper:
    def __init__(self, headless=True):
        """
        selenium twitter scraper 
        """
       
        self.headless = headless
        self.driver = None
        self.wait = None
        self.is_logged_in = False
        
        # manuel twitter giriÅŸi yapÄ±lÄ±yor
        
      
        self.disaster_keywords = [
            'deprem', 'sel', 'yangÄ±n', 'heyelan', 'Ã§Ä±ÄŸ' #ekleme yapÄ±labilir
    
        ]
        
        # Afet hashtag'leri
        self.disaster_hashtags = [
            '#deprem', '#yangÄ±n', '#afet',

        ]
        
        
        self.tweets = []
        self.is_running = False
        
      
        
    def setup_driver(self):
        """chrome driver'Ä± kurar """
        try:
            
            options = Options()
            if self.headless:
                options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            
            ua = UserAgent()
            options.add_argument(f'--user-agent={ua.random}')
            
            # driver kur
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.wait = WebDriverWait(self.driver, 10)
            print("*** chrome driver baÅŸarÄ±yla kuruldu")
            return True
            
        except Exception as e:
            print(f":(((((( driver kurulum hatasÄ±: {e}")
            return False
    
    def login_to_twitter(self):
        """twitter'a manuel giriÅŸ"""
        if not self.driver:
            if not self.setup_driver():
                return False
        
        try:
            print("--- twitter giriÅŸ sayfasÄ± aÃ§Ä±lÄ±yor...")
            print("+++ giriÅŸ yaptÄ±ktan sonra bu terminal'e dÃ¶nÃ¼n ve enter'a basÄ±n")
            
            # Twitter giriÅŸ sayfasÄ±na git
            self.driver.get("https://twitter.com/login")
            
            # Manuel giriÅŸ iÃ§in bekle
            input("+++ giriÅŸ yaptÄ±ktan sonra enter'a bas")
            
            # GiriÅŸ baÅŸarÄ±lÄ± mÄ± kontrol et
            print("--- giriÅŸ durumu kontrol ediliyor")
            
            # Ana sayfaya git
            self.driver.get("https://twitter.com/home")
            time.sleep(3)
            
            if "home" in self.driver.current_url:
                print("--- twitter'a baÅŸarÄ±yla giriÅŸ yapÄ±ldÄ±!")
                self.is_logged_in = True
                return True
            else:
                print("--- giriÅŸ durumu belirsiz, devam ediliyor")
                self.is_logged_in = True  # Manuel giriÅŸ yapÄ±ldÄ±ÄŸÄ± varsayÄ±lÄ±yor
                return True
                    
        except Exception as e:
            print(f"--- giriÅŸ hatasÄ±: {e}")
            return False
    
    def search_twitter(self, query, max_tweets=50, scroll_count=5):
        """
        twitter'da arama yapar
        
        Args:
            query (str): arama sorgusu
            max_tweets (int): maksimum tweet sayÄ±sÄ±
            scroll_count (int): sayfa kaydÄ±rma sayÄ±sÄ±
        """
        if not self.driver:
            if not self.setup_driver():
                return []
        
    
        if not self.is_logged_in:
            if not self.login_to_twitter():
                print("--- twitter'a giriÅŸ yapÄ±lamadÄ±!")
                return []
        
        try:
            search_url = f"https://twitter.com/search?q={query}&src=typed_query&f=live"
            print(f"--- twitter'da aranÄ±yor: {query}")
            print(f"--- url: {search_url}")
            
            self.driver.get(search_url)
            time.sleep(5) 
            
            # tweet'leri topla
            tweets = []
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            for scroll in range(scroll_count):
                print(f"--- sayfa {scroll + 1}/{scroll_count} kaydÄ±rÄ±lÄ±yor")
                
                
                tweet_selectors = [
                    '[data-testid="tweet"]',
                    'article[data-testid="tweet"]',
                    'div[data-testid="tweetText"]',
                    'div[lang]'
                ]
                
                tweet_elements = []
                for selector in tweet_selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            tweet_elements = elements
                            print(f"âœ… Tweet selector bulundu: {selector}")
                            break
                    except:
                        continue
                
                if not tweet_elements:
                    print("âš ï¸ Tweet elementleri bulunamadÄ±, sayfa yapÄ±sÄ± kontrol ediliyor...")
                    page_source = self.driver.page_source
                    if "tweet" in page_source.lower():
                        print("ğŸ“„ Sayfada tweet iÃ§eriÄŸi var ama selector bulunamadÄ±")
                    else:
                        print("ğŸ“„ Sayfa boÅŸ gÃ¶rÃ¼nÃ¼yor")
                
                for tweet_element in tweet_elements:
                    if len(tweets) >= max_tweets:
                        break
                    
                    try:
                        tweet_data = self._extract_tweet_data(tweet_element, query)
                        if tweet_data and tweet_data not in tweets:
                            tweets.append(tweet_data)
                            print(f"ğŸ“ Tweet Ã§ekildi: {tweet_data['text'][:50]}...")
                    except Exception as e:
                        print(f"âš ï¸ Tweet Ã§Ä±karma hatasÄ±: {e}")
                        continue
                
                # sayfayÄ± aÅŸaÄŸÄ± kaydÄ±r
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # yeni iÃ§erik yÃ¼klendi mi kontrol et
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    print("--- sayfa sonuna ulaÅŸÄ±ldÄ±")
                    break
                last_height = new_height
                
                # rate limiting iÃ§in bekle
                time.sleep(random.uniform(2, 4))
            
            print(f"--- toplam {len(tweets)} tweet Ã§ekildi")
            return tweets
            
        except Exception as e:
            print(f"--- arama hatasÄ±: {e}")
            return []
    
    def _extract_tweet_data(self, tweet_element, search_query):
        """tweet elementinden veri Ã§Ä±karÄ±r"""
        try:
            # farklÄ± tweet metni selector'larÄ±nÄ± dene
            text_selectors = [
                '[data-testid="tweetText"]',
                'div[lang]',
                'div[dir="ltr"]',
                'span'
            ]
            
            text = ""
            for selector in text_selectors:
                try:
                    text_element = tweet_element.find_element(By.CSS_SELECTOR, selector)
                    if text_element and text_element.text.strip():
                        text = text_element.text.strip()
                        break
                except:
                    continue
            
            # kullanÄ±cÄ± adÄ± ve profil URL'si
            username = "unknown"
            profile_url = None
            try:
                # KullanÄ±cÄ± adÄ± genelde kartta ilk linklerden biri, /status/ iÃ§ermeyen kullanÄ±cÄ± profiline gider
                links = tweet_element.find_elements(By.CSS_SELECTOR, 'a[role="link"]')
                for a in links:
                    href = a.get_attribute('href')
                    if href and re.match(r'^https?://(www\.)?twitter\.com/[^/]+/?$', href):
                        profile_url = href
                        # Link text'lerinden kullanÄ±cÄ± adÄ±nÄ± almayÄ± dene
                        if a.text and a.text.strip():
                            username = a.text.strip()
                        break
            except:
                pass
            
            # tweet zamanÄ± (adventure time reference
            time_element = None
            try:
                time_element = tweet_element.find_element(By.CSS_SELECTOR, 'time')
            except:
                pass
            tweet_time = time_element.get_attribute('datetime') if time_element else datetime.now().isoformat()
            
            # Tweet ID (URL'den Ã§Ä±kar)
            tweet_id = None
            try:
                tweet_link = tweet_element.find_element(By.CSS_SELECTOR, 'a[href*="/status/"]')
                href = tweet_link.get_attribute('href')
                if href:
                    tweet_id = href.split('/')[-1]
            except:
                pass
            
            # tweet'e ekli yer
            tweet_place = None
            try:
                place_link = tweet_element.find_element(By.CSS_SELECTOR, 'a[href^="/place/"]')
                if place_link and place_link.text.strip():
                    tweet_place = place_link.text.strip()
            except:
                try:
                    place_span = tweet_element.find_element(By.XPATH, ".//span[contains(@aria-label, 'Konum') or contains(@aria-label, 'Location')]")
                    if place_span and place_span.text.strip():
                        tweet_place = place_span.text.strip()
                except:
                    pass
            
            # afet ile ilgili mi kontrol et
            is_disaster_related = self._is_disaster_related(text)
            
            # metinden ÅŸehir Ã§Ä±karÄ±mÄ±
            location = detect_city_from_text(text)
            
            tweet_data = {
                'id': tweet_id,
                'text': text,
                'username': username,
                'created_at': tweet_time,
                'search_query': search_query,
                'disaster_related': is_disaster_related,
                'location': location,
                'scraping_method': 'twitter_scraper',
                'scraping_timestamp': datetime.now() #isoformat() da olabilir
            }
            
            return tweet_data
            
        except Exception as e:
            print(f"--- tweet veri Ã§Ä±karma hatasÄ±: {e}")
            return None
    
    def _is_disaster_related(self, text):
        """tweet metninin afet ile ilgili olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # anahtar kelime kontrolÃ¼
        for keyword in self.disaster_keywords:
            if keyword in text_lower:
                return True
        
        # hashtag kontrolÃ¼
        for hashtag in self.disaster_hashtags:
            if hashtag.lower() in text_lower:
                return True
        
        # acil durum kelimeleri lazÄ±m olur belki
        emergency_words = ['acil', 'kaza', 'felaket', 'emergency', 'accident', 'disaster']
        for word in emergency_words:
            if word in text_lower:
                return True
        
        return False
    
    def live_monitoring_mode(self, keywords=None, interval_seconds=30, max_tweets_per_search=20):
        """
        canlÄ± izleme modu - belirli aralÄ±klarla arama yapar
        
        Args:
            keywords (list): izlenecek anahtar kelimeler
            interval_seconds (int): arama aralÄ±ÄŸÄ± (saniye)
            max_tweets_per_search (int): her aramada Ã§ekilecek tweet sayÄ±sÄ±
        """
        if not keywords:
            keywords = self.disaster_keywords[:5] 
        
        print(f"--- canlÄ± izleme modu baÅŸlatÄ±lÄ±yor!")
        print(f"--- izlenen anahtar kelimeler: {', '.join(keywords)}")
        print(f"--- arama aralÄ±ÄŸÄ±: {interval_seconds} saniye")
        print(f"--- her aramada tweet: {max_tweets_per_search}")
        print("--" * 60)
        
        self.is_running = True
        search_count = 0
        total_tweets = 0
        disaster_tweets = 0
        
        try:
            while self.is_running:
                search_count += 1
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                print(f"\nğŸ• {current_time} - Arama #{search_count}")
                
                # anahtar kelimeleri sÄ±rayla ara
                keyword_index = (search_count - 1) % len(keywords)
                keyword = keywords[keyword_index]
                
                try:
                    tweets = self.search_twitter(
                        query=keyword,
                        max_tweets=max_tweets_per_search,
                        scroll_count=3
                    )
                    
                    if tweets:
                        # yeni tweet'leri kaydet
                        self.tweets.extend(tweets)
                        total_tweets += len(tweets)
                        
                        # afet tweet'lerini kontrol et
                        new_disaster_tweets = [t for t in tweets if t.get('disaster_related', False)]
                        disaster_tweets += len(new_disaster_tweets)
                        
                        if new_disaster_tweets:
                            print(f"ğŸš¨ {len(new_disaster_tweets)} afet tweet'i tespit edildi!")
                            for tweet in new_disaster_tweets[:3]:
                                print(f"  ğŸš¨ @{tweet['username']}: {tweet['text'][:80]}...")
                            
                            # Acil durum bildirimi
                            self._save_emergency_tweets(new_disaster_tweets)
                        
                        # istatistikleri gÃ¶ster
                        print(f"--- Ã¶zet: {total_tweets} tweet, {disaster_tweets} afet tweet")
                    
                    # batch kaydetme
                    if search_count % 5 == 0:
                        self._save_tweets_batch()
                    
                except Exception as e:
                    print(f"--- {keyword} aramasÄ±nda hata: {e}")
                    continue
                
                print(f"--- {interval_seconds} saniye bekleniyor... (durdurmak iÃ§in Ctrl+C)")
                time.sleep(interval_seconds)
                
        except KeyboardInterrupt:
            print(f"\n--- canlÄ± izleme durduruldu!")
            print(f"--- toplam: {search_count} arama, {total_tweets} tweet, {disaster_tweets} afet tweet")
            self._save_tweets_batch()
        except Exception as e:
            print(f"--- canlÄ± izleme hatasÄ±: {e}")
        finally:
            self.is_running = False
    
    def _save_emergency_tweets(self, emergency_tweets):
        """acil durum tweet'lerini ayrÄ± dosyaya kaydeder lazÄ±m olur belki"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"emergency_tweets_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(emergency_tweets, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"--- acil durum tweet'leri kaydedildi: {filename}")
            
        except Exception as e:
            print(f"--- acil durum kaydetme hatasÄ±: {e}")
    
    def _save_tweets_batch(self):
        """tweet batch'ini dosyaya kaydeder"""
        try:
            if not self.tweets:
                return
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"selenium_tweets_batch_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.tweets, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ’¾ {len(self.tweets)} tweet {filename} dosyasÄ±na kaydedildi")
            
            # belleÄŸi temizle
            self.tweets = []
            
        except Exception as e:
            print(f"--- batch kaydetme hatasÄ±: {e}")
    
    def search_disaster_tweets(self, keyword="deprem", count=50):
        """belirli bir afet anahtar kelimesi iÃ§in tweet'leri arar"""
        return self.search_twitter(query=keyword, max_tweets=count)
    
    def search_multiple_keywords(self, keywords, tweets_per_keyword=20):
        """birden fazla anahtar kelime iÃ§in arama yapar"""
        all_tweets = []
        
        for keyword in keywords:
            print(f"ğŸ” {keyword} aranÄ±yor...")
            tweets = self.search_twitter(query=keyword, max_tweets=tweets_per_keyword)
            all_tweets.extend(tweets)
            
            # rate limiting iÃ§in bekle
            time.sleep(random.uniform(2, 5))
        
        return all_tweets
    
    def close(self):
        """driver'Ä± kapatÄ±r"""
        if self.driver:
            self.driver.quit()
            print("--- driver kapatÄ±ldÄ±")

def main():
    """ana fonksiyon"""
    print("--- selenium tabanlÄ± twitter afet scraper")
    print("--" * 50)
    
    try:
        # scraper'Ä± baÅŸlat
        scraper = SeleniumTwitterScraper(headless=False)
        
        print("--- mod seÃ§in:")
        print("1. tek arama")
        print("2. Ã§oklu anahtar kelime arama")
        print("3. canlÄ± izleme modu")
        print("4. twitter'a giriÅŸ yap")
        
        choice = input("\nseÃ§iminiz (1-5): ").strip()
        
        if choice == "1":
            keyword = input("arama anahtar kelimesi (varsayÄ±lan: deprem): ").strip() or "deprem"
            count = input("tweet sayÄ±sÄ± (varsayÄ±lan: 50): ").strip()
            count = int(count) if count.isdigit() else 50
            
            tweets = scraper.search_disaster_tweets(keyword, count)
            
        elif choice == "2":
            keywords_input = input("Anahtar kelimeler (virgÃ¼lle ayÄ±rÄ±n): ").strip()
            keywords = [k.strip() for k in keywords_input.split(',')] if keywords_input else ['deprem', 'sel', 'yangÄ±n']
            tweets_per_keyword = input("Her anahtar kelime iÃ§in tweet sayÄ±sÄ± (varsayÄ±lan: 20): ").strip()
            tweets_per_keyword = int(tweets_per_keyword) if tweets_per_keyword.isdigit() else 20
            
            tweets = scraper.search_multiple_keywords(keywords, tweets_per_keyword)
            
        elif choice == "3":
            print("--- canlÄ± izleme modu seÃ§ildi!")
            keywords_input = input("izlenecek anahtar kelimeler (virgÃ¼lle ayÄ±rÄ±n, boÅŸ=varsayÄ±lan): ").strip()
            keywords = [k.strip() for k in keywords_input.split(',')] if keywords_input else None
            
            interval = input("arama aralÄ±ÄŸÄ± (saniye, varsayÄ±lan: 30): ").strip()
            interval = int(interval) if interval.isdigit() else 30
            
            max_tweets = input("her aramada tweet sayÄ±sÄ± (varsayÄ±lan: 20): ").strip()
            max_tweets = int(max_tweets) if max_tweets.isdigit() else 20
            
            scraper.live_monitoring_mode(keywords, interval, max_tweets)
            return
            
        elif choice == "4":
            print("--- twitter'a giriÅŸ yapÄ±lÄ±yor...")
            if scraper.login_to_twitter():
                print("--- giriÅŸ baÅŸarÄ±lÄ±! ÅŸimdi arama yapabilirsiniz.")
            else:
                print("--- giriÅŸ baÅŸarÄ±sÄ±z!")
            return
            
        else:
            print("--- geÃ§ersiz seÃ§im, varsayÄ±lan olarak deprem aramasÄ± yapÄ±lacak")
            tweets = scraper.search_disaster_tweets("deprem", 50)
        
        if tweets:
            # tweet'leri kaydet
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"selenium_tweets_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(tweets, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\n--- Ã¶zet:")
            print(f"  â€¢ toplam tweet: {len(tweets)}")
            print(f"  â€¢ kayÄ±t dosyasÄ±: {filename}") 
            
          
            disaster_count = sum(1 for tweet in tweets if tweet.get('disaster_related', False))
            print(f"  â€¢ afet ile ilgili tweet: {disaster_count}")
            
          
        
    except Exception as e:
        print(f"--- hata oluÅŸtu: {e}")
    finally:
        # driver'Ä± kapat
        if 'scraper' in locals():
            scraper.close()

if __name__ == "__main__":
    main()



